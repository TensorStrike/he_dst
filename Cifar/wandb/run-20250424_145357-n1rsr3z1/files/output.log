


================================================================================

Iteration start: 1/1

==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
model ResNet50
Growth mode: global_gradients not supported!
Supported modes are: ['random', 'momentum', 'momentum_neuron', 'gradient']
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (classifier): Linear(in_features=2048, out_features=100, bias=False)
)
add module
Removing biases...
Removed 53 layers.
Removing 2D batch norms...
Removing bn1 of size torch.Size([64]) = 64 parameters.
Removing layer1.0.bn1 of size torch.Size([64]) = 64 parameters.
Removing layer1.0.bn2 of size torch.Size([64]) = 64 parameters.
Removing layer1.0.bn3 of size torch.Size([256]) = 256 parameters.
Removing layer1.0.shortcut.1 of size torch.Size([256]) = 256 parameters.
Removing layer1.1.bn1 of size torch.Size([64]) = 64 parameters.
Removing layer1.1.bn2 of size torch.Size([64]) = 64 parameters.
Removing layer1.1.bn3 of size torch.Size([256]) = 256 parameters.
Removing layer1.2.bn1 of size torch.Size([64]) = 64 parameters.
Removing layer1.2.bn2 of size torch.Size([64]) = 64 parameters.
Removing layer1.2.bn3 of size torch.Size([256]) = 256 parameters.
Removing layer2.0.bn1 of size torch.Size([128]) = 128 parameters.
Removing layer2.0.bn2 of size torch.Size([128]) = 128 parameters.
Removing layer2.0.bn3 of size torch.Size([512]) = 512 parameters.
Removing layer2.0.shortcut.1 of size torch.Size([512]) = 512 parameters.
Removing layer2.1.bn1 of size torch.Size([128]) = 128 parameters.
Removing layer2.1.bn2 of size torch.Size([128]) = 128 parameters.
Removing layer2.1.bn3 of size torch.Size([512]) = 512 parameters.
Removing layer2.2.bn1 of size torch.Size([128]) = 128 parameters.
Removing layer2.2.bn2 of size torch.Size([128]) = 128 parameters.
Removing layer2.2.bn3 of size torch.Size([512]) = 512 parameters.
Removing layer2.3.bn1 of size torch.Size([128]) = 128 parameters.
Removing layer2.3.bn2 of size torch.Size([128]) = 128 parameters.
Removing layer2.3.bn3 of size torch.Size([512]) = 512 parameters.
Removing layer3.0.bn1 of size torch.Size([256]) = 256 parameters.
Removing layer3.0.bn2 of size torch.Size([256]) = 256 parameters.
Removing layer3.0.bn3 of size torch.Size([1024]) = 1024 parameters.
Removing layer3.0.shortcut.1 of size torch.Size([1024]) = 1024 parameters.
Removing layer3.1.bn1 of size torch.Size([256]) = 256 parameters.
Removing layer3.1.bn2 of size torch.Size([256]) = 256 parameters.
Removing layer3.1.bn3 of size torch.Size([1024]) = 1024 parameters.
Removing layer3.2.bn1 of size torch.Size([256]) = 256 parameters.
Removing layer3.2.bn2 of size torch.Size([256]) = 256 parameters.
Removing layer3.2.bn3 of size torch.Size([1024]) = 1024 parameters.
Removing layer3.3.bn1 of size torch.Size([256]) = 256 parameters.
Removing layer3.3.bn2 of size torch.Size([256]) = 256 parameters.
Removing layer3.3.bn3 of size torch.Size([1024]) = 1024 parameters.
Removing layer3.4.bn1 of size torch.Size([256]) = 256 parameters.
Removing layer3.4.bn2 of size torch.Size([256]) = 256 parameters.
Removing layer3.4.bn3 of size torch.Size([1024]) = 1024 parameters.
Removing layer3.5.bn1 of size torch.Size([256]) = 256 parameters.
Removing layer3.5.bn2 of size torch.Size([256]) = 256 parameters.
Removing layer3.5.bn3 of size torch.Size([1024]) = 1024 parameters.
Removing layer4.0.bn1 of size torch.Size([512]) = 512 parameters.
Removing layer4.0.bn2 of size torch.Size([512]) = 512 parameters.
Removing layer4.0.bn3 of size torch.Size([2048]) = 2048 parameters.
Removing layer4.0.shortcut.1 of size torch.Size([2048]) = 2048 parameters.
Removing layer4.1.bn1 of size torch.Size([512]) = 512 parameters.
Removing layer4.1.bn2 of size torch.Size([512]) = 512 parameters.
Removing layer4.1.bn3 of size torch.Size([2048]) = 2048 parameters.
Removing layer4.2.bn1 of size torch.Size([512]) = 512 parameters.
Removing layer4.2.bn2 of size torch.Size([512]) = 512 parameters.
Removing layer4.2.bn3 of size torch.Size([2048]) = 2048 parameters.
Removing 1D batch norms...
baseline fitler num 7552
initialize by fixed_ERK
Sparsity of var:conv1.weight had to be set to 0.
Sparsity of var:layer1.0.conv1.weight had to be set to 0.
layer: conv1.weight, shape: torch.Size([64, 3, 3, 3]), density: 1.0
layer: layer1.0.conv1.weight, shape: torch.Size([64, 64, 1, 1]), density: 1.0
layer: layer1.0.conv2.weight, shape: torch.Size([64, 64, 3, 3]), density: 0.16747319723795276
layer: layer1.0.conv3.weight, shape: torch.Size([256, 64, 1, 1]), density: 0.9054800850663938
layer: layer1.0.shortcut.0.weight, shape: torch.Size([256, 64, 1, 1]), density: 0.9054800850663938
layer: layer1.1.conv1.weight, shape: torch.Size([64, 256, 1, 1]), density: 0.9054800850663938
layer: layer1.1.conv2.weight, shape: torch.Size([64, 64, 3, 3]), density: 0.16747319723795276
layer: layer1.1.conv3.weight, shape: torch.Size([256, 64, 1, 1]), density: 0.9054800850663938
layer: layer1.2.conv1.weight, shape: torch.Size([64, 256, 1, 1]), density: 0.9054800850663938
layer: layer1.2.conv2.weight, shape: torch.Size([64, 64, 3, 3]), density: 0.16747319723795276
layer: layer1.2.conv3.weight, shape: torch.Size([256, 64, 1, 1]), density: 0.9054800850663938
layer: layer2.0.conv1.weight, shape: torch.Size([128, 256, 1, 1]), density: 0.5427256410491118
layer: layer2.0.conv2.weight, shape: torch.Size([128, 128, 3, 3]), density: 0.08186189864989481
layer: layer2.0.conv3.weight, shape: torch.Size([512, 128, 1, 1]), density: 0.4513340175563857
layer: layer2.0.shortcut.0.weight, shape: torch.Size([512, 256, 1, 1]), density: 0.2706598080361503
layer: layer2.1.conv1.weight, shape: torch.Size([128, 512, 1, 1]), density: 0.4513340175563857
layer: layer2.1.conv2.weight, shape: torch.Size([128, 128, 3, 3]), density: 0.08186189864989481
layer: layer2.1.conv3.weight, shape: torch.Size([512, 128, 1, 1]), density: 0.4513340175563857
layer: layer2.2.conv1.weight, shape: torch.Size([128, 512, 1, 1]), density: 0.4513340175563857
layer: layer2.2.conv2.weight, shape: torch.Size([128, 128, 3, 3]), density: 0.08186189864989481
layer: layer2.2.conv3.weight, shape: torch.Size([512, 128, 1, 1]), density: 0.4513340175563857
layer: layer2.3.conv1.weight, shape: torch.Size([128, 512, 1, 1]), density: 0.4513340175563857
layer: layer2.3.conv2.weight, shape: torch.Size([128, 128, 3, 3]), density: 0.08186189864989481
layer: layer2.3.conv3.weight, shape: torch.Size([512, 128, 1, 1]), density: 0.4513340175563857
layer: layer3.0.conv1.weight, shape: torch.Size([256, 512, 1, 1]), density: 0.2706598080361503
layer: layer3.0.conv2.weight, shape: torch.Size([256, 256, 3, 3]), density: 0.04046227433267702
layer: layer3.0.conv3.weight, shape: torch.Size([1024, 256, 1, 1]), density: 0.22531550253399008
layer: layer3.0.shortcut.0.weight, shape: torch.Size([1024, 512, 1, 1]), density: 0.13515415089597377
layer: layer3.1.conv1.weight, shape: torch.Size([256, 1024, 1, 1]), density: 0.22531550253399008
layer: layer3.1.conv2.weight, shape: torch.Size([256, 256, 3, 3]), density: 0.04046227433267702
layer: layer3.1.conv3.weight, shape: torch.Size([1024, 256, 1, 1]), density: 0.22531550253399008
layer: layer3.2.conv1.weight, shape: torch.Size([256, 1024, 1, 1]), density: 0.22531550253399008
layer: layer3.2.conv2.weight, shape: torch.Size([256, 256, 3, 3]), density: 0.04046227433267702
layer: layer3.2.conv3.weight, shape: torch.Size([1024, 256, 1, 1]), density: 0.22531550253399008
layer: layer3.3.conv1.weight, shape: torch.Size([256, 1024, 1, 1]), density: 0.22531550253399008
layer: layer3.3.conv2.weight, shape: torch.Size([256, 256, 3, 3]), density: 0.04046227433267702
layer: layer3.3.conv3.weight, shape: torch.Size([1024, 256, 1, 1]), density: 0.22531550253399008
layer: layer3.4.conv1.weight, shape: torch.Size([256, 1024, 1, 1]), density: 0.22531550253399008
layer: layer3.4.conv2.weight, shape: torch.Size([256, 256, 3, 3]), density: 0.04046227433267702
layer: layer3.4.conv3.weight, shape: torch.Size([1024, 256, 1, 1]), density: 0.22531550253399008
layer: layer3.5.conv1.weight, shape: torch.Size([256, 1024, 1, 1]), density: 0.22531550253399008
layer: layer3.5.conv2.weight, shape: torch.Size([256, 256, 3, 3]), density: 0.04046227433267702
layer: layer3.5.conv3.weight, shape: torch.Size([1024, 256, 1, 1]), density: 0.22531550253399008
layer: layer4.0.conv1.weight, shape: torch.Size([512, 1024, 1, 1]), density: 0.13515415089597377
layer: layer4.0.conv2.weight, shape: torch.Size([512, 512, 3, 3]), density: 0.02011396841827091
layer: layer4.0.conv3.weight, shape: torch.Size([2048, 512, 1, 1]), density: 0.11256987470594433
layer: layer4.0.shortcut.0.weight, shape: torch.Size([2048, 1024, 1, 1]), density: 0.06753313716746152
layer: layer4.1.conv1.weight, shape: torch.Size([512, 2048, 1, 1]), density: 0.11256987470594433
layer: layer4.1.conv2.weight, shape: torch.Size([512, 512, 3, 3]), density: 0.02011396841827091
layer: layer4.1.conv3.weight, shape: torch.Size([2048, 512, 1, 1]), density: 0.11256987470594433
layer: layer4.2.conv1.weight, shape: torch.Size([512, 2048, 1, 1]), density: 0.11256987470594433
layer: layer4.2.conv2.weight, shape: torch.Size([512, 512, 3, 3]), density: 0.02011396841827091
layer: layer4.2.conv3.weight, shape: torch.Size([2048, 512, 1, 1]), density: 0.11256987470594433
layer: classifier.weight, shape: torch.Size([100, 2048]), density: 0.48322266403046304
Overall sparsity 0.10000000000000005
conv1.weight 1728
layer1.0.conv1.weight 4096
layer1.0.conv2.weight 36864
layer1.0.conv3.weight 16384
layer1.0.shortcut.0.weight 16384
layer1.1.conv1.weight 16384
layer1.1.conv2.weight 36864
layer1.1.conv3.weight 16384
layer1.2.conv1.weight 16384
layer1.2.conv2.weight 36864
layer1.2.conv3.weight 16384
layer2.0.conv1.weight 32768
layer2.0.conv2.weight 147456
layer2.0.conv3.weight 65536
layer2.0.shortcut.0.weight 131072
layer2.1.conv1.weight 65536
layer2.1.conv2.weight 147456
layer2.1.conv3.weight 65536
layer2.2.conv1.weight 65536
layer2.2.conv2.weight 147456
layer2.2.conv3.weight 65536
layer2.3.conv1.weight 65536
layer2.3.conv2.weight 147456
layer2.3.conv3.weight 65536
layer3.0.conv1.weight 131072
layer3.0.conv2.weight 589824
layer3.0.conv3.weight 262144
layer3.0.shortcut.0.weight 524288
layer3.1.conv1.weight 262144
layer3.1.conv2.weight 589824
layer3.1.conv3.weight 262144
layer3.2.conv1.weight 262144
layer3.2.conv2.weight 589824
layer3.2.conv3.weight 262144
layer3.3.conv1.weight 262144
layer3.3.conv2.weight 589824
layer3.3.conv3.weight 262144
layer3.4.conv1.weight 262144
layer3.4.conv2.weight 589824
layer3.4.conv3.weight 262144
layer3.5.conv1.weight 262144
layer3.5.conv2.weight 589824
layer3.5.conv3.weight 262144
layer4.0.conv1.weight 524288
layer4.0.conv2.weight 2359296
layer4.0.conv3.weight 1048576
layer4.0.shortcut.0.weight 2097152
layer4.1.conv1.weight 1048576
layer4.1.conv2.weight 2359296
layer4.1.conv3.weight 1048576
layer4.2.conv1.weight 1048576
layer4.2.conv2.weight 2359296
layer4.2.conv3.weight 1048576
classifier.weight 204800
Total Model parameters after init: 2367257 23652032
Total parameters under sparsity level of 0.1: 0.10008683397688621
save_dir ./Chase_models/saved_models/test/density_0.1/stop_gmp_epochs_130/epoch_160/layer_interval_1000/start_layer_rate_0.5
=====================================
begin pre training
Train Epoch: 0 [0/50000 (0%)]	Loss: 4.644441 Accuracy: 0/100 (0.000%
Train Epoch: 0 [10000/50000 (20%)]	Loss: 4.587985 Accuracy: 129/10100 (1.277%
Train Epoch: 0 [20000/50000 (40%)]	Loss: 4.262469 Accuracy: 366/20100 (1.821%
Train Epoch: 0 [30000/50000 (60%)]	Loss: 4.176337 Accuracy: 775/30100 (2.575%
Train Epoch: 0 [40000/50000 (80%)]	Loss: 4.126044 Accuracy: 1378/40100 (3.436%
current layer rate 0.011449931725079632
===========del layer===============
===========del layer with layer-wise HE===============
Total filter_names items: 7552.0
Total filter_masks items: 20676608.0
7552.0 / 7552 channels
Total channels to prune: 86.46988438780136
Layer ('layer2', 0, 'conv1'): Pruned 1/128.0 channels based on HE
Layer ('layer2', 0, 'conv2'): Pruned 1/128.0 channels based on HE
Layer ('layer2', 1, 'conv1'): Pruned 1/128.0 channels based on HE
Layer ('layer2', 1, 'conv2'): Pruned 1/128.0 channels based on HE
Layer ('layer2', 2, 'conv1'): Pruned 1/128.0 channels based on HE
Layer ('layer2', 2, 'conv2'): Pruned 1/128.0 channels based on HE
Layer ('layer2', 3, 'conv1'): Pruned 1/128.0 channels based on HE
Layer ('layer2', 3, 'conv2'): Pruned 1/128.0 channels based on HE
Layer ('layer3', 0, 'conv1'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 0, 'conv2'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 1, 'conv1'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 1, 'conv2'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 2, 'conv1'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 2, 'conv2'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 3, 'conv1'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 3, 'conv2'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 4, 'conv1'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 4, 'conv2'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 5, 'conv1'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 5, 'conv2'): Pruned 2/256.0 channels based on HE
Layer ('layer4', 0, 'conv1'): Pruned 5/512.0 channels based on HE
Layer ('layer4', 0, 'conv2'): Pruned 5/512.0 channels based on HE
Layer ('layer4', 1, 'conv1'): Pruned 5/512.0 channels based on HE
Layer ('layer4', 1, 'conv2'): Pruned 5/512.0 channels based on HE
Layer ('layer4', 2, 'conv1'): Pruned 5/512.0 channels based on HE
Layer ('layer4', 2, 'conv2'): Pruned 5/512.0 channels based on HE
update_filter_mask
After update_filter_mask:
Total filter_names items: 7490.0
Total filter_masks items: 20391071.0
After apply_mask:
Total filter_names items: 7490.0
Total filter_masks items: 20391071.0
Total pruned: 62/86.46988438780136 channels
===========done ===============

Training summary: Average loss: 4.4515, Accuracy: 2006/50000 (4.012%)


Evaluation: Average loss: 3.9182, Accuracy: 855/10000 (8.550%)

Current learning rate: 0.1. Time taken for epoch: 91.77 seconds.

Train Epoch: 1 [0/50000 (0%)]	Loss: 3.717634 Accuracy: 14/100 (14.000%
Train Epoch: 1 [10000/50000 (20%)]	Loss: 3.853412 Accuracy: 860/10100 (8.515%
Train Epoch: 1 [20000/50000 (40%)]	Loss: 3.806626 Accuracy: 1780/20100 (8.856%
Train Epoch: 1 [30000/50000 (60%)]	Loss: 3.613328 Accuracy: 2821/30100 (9.372%
Train Epoch: 1 [40000/50000 (80%)]	Loss: 3.690410 Accuracy: 3990/40100 (9.950%
current mest  death_rate 0.14997080567080823


dynamic sparse change prune
to kill -17286.200000001118 expect 2365203.200000001
conv1.weight, mask/weight parameters,1728,1728 density 1.0,1.0
before tensor 4096 mask 4096.0 4096
layer1.0.conv1.weight, mask/weight parameters,4096,4096 density 1.0,1.0
before tensor 36864 mask 6206.0 36864
layer1.0.conv2.weight, mask/weight parameters,6206,6206 density 0.16834852430555555,0.16834852430555555
before tensor 16384 mask 14835.0 16384
layer1.0.conv3.weight, mask/weight parameters,14835,14835 density 0.90545654296875,0.90545654296875
layer1.0.shortcut.0.weight, mask/weight parameters,14829,14829 density 0.90509033203125,0.90509033203125
before tensor 16384 mask 14823.0 16384
layer1.1.conv1.weight, mask/weight parameters,14823,14823 density 0.90472412109375,0.90472412109375
before tensor 36864 mask 6155.0 36864
layer1.1.conv2.weight, mask/weight parameters,6155,6155 density 0.1669650607638889,0.1669650607638889
before tensor 16384 mask 14857.0 16384
layer1.1.conv3.weight, mask/weight parameters,14857,14857 density 0.90679931640625,0.90679931640625
before tensor 16384 mask 14849.0 16384
layer1.2.conv1.weight, mask/weight parameters,14849,14849 density 0.90631103515625,0.90631103515625
before tensor 36864 mask 6104.0 36864
layer1.2.conv2.weight, mask/weight parameters,6104,6104 density 0.1655815972222222,0.1655815972222222
before tensor 16384 mask 14785.0 16384
layer1.2.conv3.weight, mask/weight parameters,14785,14785 density 0.90240478515625,0.90240478515625
before tensor 32768 mask 17693.0 32768
layer2.0.conv1.weight, mask/weight parameters,17693,17693 density 0.54419906496063,0.54419906496063
before tensor 147456 mask 11928.0 147456
layer2.0.conv2.weight, mask/weight parameters,11928,11928 density 0.08217083100832868,0.08217083100832868
before tensor 65536 mask 29304.0 65536
layer2.0.conv3.weight, mask/weight parameters,29304,29304 density 0.45066437007874016,0.45066437007874016
layer2.0.shortcut.0.weight, mask/weight parameters,35383,35383 density 0.26995086669921875,0.26995086669921875
before tensor 65536 mask 29306.0 65536
layer2.1.conv1.weight, mask/weight parameters,29306,29306 density 0.4506951279527559,0.4506951279527559
before tensor 147456 mask 11905.0 147456
layer2.1.conv2.weight, mask/weight parameters,11905,11905 density 0.08201238624699472,0.08201238624699472
before tensor 65536 mask 29358.0 65536
layer2.1.conv3.weight, mask/weight parameters,29358,29358 density 0.45149483267716534,0.45149483267716534
before tensor 65536 mask 29562.0 65536
layer2.2.conv1.weight, mask/weight parameters,29562,29562 density 0.45463213582677164,0.45463213582677164
before tensor 147456 mask 11910.0 147456
layer2.2.conv2.weight, mask/weight parameters,11910,11910 density 0.08204683076032819,0.08204683076032819
before tensor 65536 mask 29292.0 65536
layer2.2.conv3.weight, mask/weight parameters,29292,29292 density 0.45047982283464566,0.45047982283464566
before tensor 65536 mask 29323.0 65536
layer2.3.conv1.weight, mask/weight parameters,29323,29323 density 0.45095656988188976,0.45095656988188976
before tensor 147456 mask 11963.0 147456
layer2.3.conv2.weight, mask/weight parameters,11963,11963 density 0.08241194260166299,0.08241194260166299
before tensor 65536 mask 29458.0 65536
layer2.3.conv3.weight, mask/weight parameters,29458,29458 density 0.4530327263779528,0.4530327263779528
before tensor 131072 mask 35238.0 131072
layer3.0.conv1.weight, mask/weight parameters,35238,35238 density 0.2709614911417323,0.2709614911417323
before tensor 589824 mask 23563.0 589824
layer3.0.conv2.weight, mask/weight parameters,23563,23563 density 0.04058080338382899,0.04058080338382899
before tensor 262144 mask 58659.0 262144
layer3.0.conv3.weight, mask/weight parameters,58659,58659 density 0.22552826648622049,0.22552826648622049
layer3.0.shortcut.0.weight, mask/weight parameters,70817,70817 density 0.1350727081298828,0.1350727081298828
before tensor 262144 mask 58952.0 262144
layer3.1.conv1.weight, mask/weight parameters,58952,58952 density 0.22665477362204725,0.22665477362204725
before tensor 589824 mask 23607.0 589824
layer3.1.conv2.weight, mask/weight parameters,23607,23607 density 0.040656581313162625,0.040656581313162625
before tensor 262144 mask 58443.0 262144
layer3.1.conv3.weight, mask/weight parameters,58443,58443 density 0.22469780388779528,0.22469780388779528
before tensor 262144 mask 58207.0 262144
layer3.2.conv1.weight, mask/weight parameters,58207,58207 density 0.2237904466043307,0.2237904466043307
before tensor 589824 mask 23221.0 589824
layer3.2.conv2.weight, mask/weight parameters,23221,23221 density 0.03999180220582663,0.03999180220582663
before tensor 262144 mask 58814.0 262144
layer3.2.conv3.weight, mask/weight parameters,58814,58814 density 0.22612420029527558,0.22612420029527558
before tensor 262144 mask 58709.0 262144
layer3.3.conv1.weight, mask/weight parameters,58709,58709 density 0.2257205031988189,0.2257205031988189
before tensor 589824 mask 23636.0 589824
layer3.3.conv2.weight, mask/weight parameters,23636,23636 density 0.04070652585749616,0.04070652585749616
before tensor 262144 mask 58967.0 262144
layer3.3.conv3.weight, mask/weight parameters,58967,58967 density 0.22671244463582677,0.22671244463582677
before tensor 262144 mask 58595.0 262144
layer3.4.conv1.weight, mask/weight parameters,58595,58595 density 0.2252822034940945,0.2252822034940945
before tensor 589824 mask 23532.0 589824
layer3.4.conv2.weight, mask/weight parameters,23532,23532 density 0.04052741438816211,0.04052741438816211
before tensor 262144 mask 58116.0 262144
layer3.4.conv3.weight, mask/weight parameters,58116,58116 density 0.22344057578740156,0.22344057578740156
before tensor 262144 mask 58624.0 262144
layer3.5.conv1.weight, mask/weight parameters,58624,58624 density 0.22539370078740156,0.22539370078740156
before tensor 589824 mask 23544.0 589824
layer3.5.conv2.weight, mask/weight parameters,23544,23544 density 0.04054808109616219,0.04054808109616219
before tensor 262144 mask 58648.0 262144
layer3.5.conv3.weight, mask/weight parameters,58648,58648 density 0.2254859744094488,0.2254859744094488
before tensor 524288 mask 70094.0 524288
layer4.0.conv1.weight, mask/weight parameters,70094,70094 density 0.1350121733234714,0.1350121733234714
before tensor 2359296 mask 47057.0 2359296
layer4.0.conv2.weight, mask/weight parameters,47057,47057 density 0.020340695958963293,0.020340695958963293
before tensor 1048576 mask 117013.0 1048576
layer4.0.conv3.weight, mask/weight parameters,117013,117013 density 0.11269280849358974,0.11269280849358974
layer4.0.shortcut.0.weight, mask/weight parameters,142133,142133 density 0.06777429580688477,0.06777429580688477
before tensor 1048576 mask 117412.0 1048576
layer4.1.conv1.weight, mask/weight parameters,117412,117412 density 0.11307707716962524,0.11307707716962524
before tensor 2359296 mask 46350.0 2359296
layer4.1.conv2.weight, mask/weight parameters,46350,46350 density 0.02003509058584161,0.02003509058584161
before tensor 1048576 mask 116893.0 1048576
layer4.1.conv3.weight, mask/weight parameters,116893,116893 density 0.11257723896696252,0.11257723896696252
before tensor 1048576 mask 116721.0 1048576
layer4.2.conv1.weight, mask/weight parameters,116721,116721 density 0.11241158931213018,0.11241158931213018
before tensor 2359296 mask 46590.0 2359296
layer4.2.conv2.weight, mask/weight parameters,46590,46590 density 0.020138832155218136,0.020138832155218136
before tensor 1048576 mask 117232.0 1048576
layer4.2.conv3.weight, mask/weight parameters,117232,117232 density 0.11290372287968442,0.11290372287968442
classifier.weight, mask/weight parameters,98878,98878 density 0.482802734375,0.482802734375
Total Model parameters after dst: 2347917 2347917
Total parameters under density level of 0.1: 0.10048220753690273 0.10048220753690273 after dst
Death rate: 0.14997080567080823

dynamic sparse change grow
self.total_params*pruning_rate 3547114.2947917376
self.total_nonzero 2347917.0
to grow 1199197.2947917376
len(all_scores) 23366495 21018057
increse 1199197.0 before_mask 3547113.0 after_mask 3547113.0
total_nonzero_new None
conv1.weight, mask/weight parameters,1728,1728 density 1.0,1.0
before tensor 4096 mask 4096.0 4096
layer1.0.conv1.weight, mask/weight parameters,4096,4096 density 1.0,1.0
before tensor 36864 mask 8525.0 36864
layer1.0.conv2.weight, mask/weight parameters,8525,6206 density 0.2312554253472222,0.16834852430555555
before tensor 16384 mask 14961.0 16384
layer1.0.conv3.weight, mask/weight parameters,14961,14835 density 0.91314697265625,0.90545654296875
layer1.0.shortcut.0.weight, mask/weight parameters,15002,14829 density 0.9156494140625,0.90509033203125
before tensor 16384 mask 14861.0 16384
layer1.1.conv1.weight, mask/weight parameters,14861,14823 density 0.90704345703125,0.90472412109375
before tensor 36864 mask 7033.0 36864
layer1.1.conv2.weight, mask/weight parameters,7033,6155 density 0.19078233506944445,0.1669650607638889
before tensor 16384 mask 14905.0 16384
layer1.1.conv3.weight, mask/weight parameters,14905,14857 density 0.90972900390625,0.90679931640625
before tensor 16384 mask 14867.0 16384
layer1.2.conv1.weight, mask/weight parameters,14867,14849 density 0.90740966796875,0.90631103515625
before tensor 36864 mask 7159.0 36864
layer1.2.conv2.weight, mask/weight parameters,7159,6104 density 0.19420030381944445,0.1655815972222222
before tensor 16384 mask 14806.0 16384
layer1.2.conv3.weight, mask/weight parameters,14806,14785 density 0.9036865234375,0.90240478515625
before tensor 32768 mask 18400.0 32768
layer2.0.conv1.weight, mask/weight parameters,18400,17693 density 0.5659448818897638,0.54419906496063
before tensor 147456 mask 17727.0 147456
layer2.0.conv2.weight, mask/weight parameters,17727,11928 density 0.12211957757248848,0.08217083100832868
before tensor 65536 mask 30608.0 65536
layer2.0.conv3.weight, mask/weight parameters,30608,29304 density 0.47071850393700787,0.45066437007874016
layer2.0.shortcut.0.weight, mask/weight parameters,41401,35383 density 0.31586456298828125,0.26995086669921875
before tensor 65536 mask 30024.0 65536
layer2.1.conv1.weight, mask/weight parameters,30024,29306 density 0.46173720472440943,0.4506951279527559
before tensor 147456 mask 18911.0 147456
layer2.1.conv2.weight, mask/weight parameters,18911,11905 density 0.13027603832985443,0.08201238624699472
before tensor 65536 mask 30456.0 65536
layer2.1.conv3.weight, mask/weight parameters,30456,29358 density 0.468380905511811,0.45149483267716534
before tensor 65536 mask 30127.0 65536
layer2.2.conv1.weight, mask/weight parameters,30127,29562 density 0.4633212352362205,0.45463213582677164
before tensor 147456 mask 14713.0 147456
layer2.2.conv2.weight, mask/weight parameters,14713,11910 density 0.10135642493507209,0.08204683076032819
before tensor 65536 mask 29687.0 65536
layer2.2.conv3.weight, mask/weight parameters,29687,29292 density 0.4565545029527559,0.45047982283464566
before tensor 65536 mask 29398.0 65536
layer2.3.conv1.weight, mask/weight parameters,29398,29323 density 0.4521099901574803,0.45095656988188976
before tensor 147456 mask 13122.0 147456
layer2.3.conv2.weight, mask/weight parameters,13122,11963 density 0.09039618079236159,0.08241194260166299
before tensor 65536 mask 29541.0 65536
layer2.3.conv3.weight, mask/weight parameters,29541,29458 density 0.4543091781496063,0.4530327263779528
before tensor 131072 mask 37935.0 131072
layer3.0.conv1.weight, mask/weight parameters,37935,35238 density 0.2916999876968504,0.2709614911417323
before tensor 589824 mask 44755.0 589824
layer3.0.conv2.weight, mask/weight parameters,44755,23563 density 0.07707820971197497,0.04058080338382899
before tensor 262144 mask 62848.0 262144
layer3.0.conv3.weight, mask/weight parameters,62848,58659 density 0.24163385826771652,0.22552826648622049
layer3.0.shortcut.0.weight, mask/weight parameters,104136,70817 density 0.1986236572265625,0.1350727081298828
before tensor 262144 mask 68918.0 262144
layer3.1.conv1.weight, mask/weight parameters,68918,58952 density 0.26497139517716534,0.22665477362204725
before tensor 589824 mask 122981.0 589824
layer3.1.conv2.weight, mask/weight parameters,122981,23607 density 0.21180103471318054,0.040656581313162625
before tensor 262144 mask 77964.0 262144
layer3.1.conv3.weight, mask/weight parameters,77964,58443 density 0.29975086122047245,0.22469780388779528
before tensor 262144 mask 65855.0 262144
layer3.2.conv1.weight, mask/weight parameters,65855,58207 density 0.2531949741633858,0.2237904466043307
before tensor 589824 mask 70319.0 589824
layer3.2.conv2.weight, mask/weight parameters,70319,23221 density 0.12110518665481776,0.03999180220582663
before tensor 262144 mask 65501.0 262144
layer3.2.conv3.weight, mask/weight parameters,65501,58814 density 0.251833938238189,0.22612420029527558
before tensor 262144 mask 65604.0 262144
layer3.3.conv1.weight, mask/weight parameters,65604,58709 density 0.2522299458661417,0.2257205031988189
before tensor 589824 mask 70208.0 589824
layer3.3.conv2.weight, mask/weight parameters,70208,23636 density 0.12091401960581699,0.04070652585749616
before tensor 262144 mask 64892.0 262144
layer3.3.conv3.weight, mask/weight parameters,64892,58967 density 0.24949249507874016,0.22671244463582677
before tensor 262144 mask 63769.0 262144
layer3.4.conv1.weight, mask/weight parameters,63769,58595 density 0.24517485851377951,0.2252822034940945
before tensor 589824 mask 55206.0 589824
layer3.4.conv2.weight, mask/weight parameters,55206,23532 density 0.0950771901543803,0.04052741438816211
before tensor 262144 mask 61582.0 262144
layer3.4.conv3.weight, mask/weight parameters,61582,58116 density 0.23676642470472442,0.22344057578740156
before tensor 262144 mask 60291.0 262144
layer3.5.conv1.weight, mask/weight parameters,60291,58624 density 0.23180287278543307,0.22539370078740156
before tensor 589824 mask 35742.0 589824
layer3.5.conv2.weight, mask/weight parameters,35742,23544 density 0.061555789778246224,0.04054808109616219
before tensor 262144 mask 59875.0 262144
layer3.5.conv3.weight, mask/weight parameters,59875,58648 density 0.23020346333661418,0.2254859744094488
before tensor 524288 mask 73774.0 524288
layer4.0.conv1.weight, mask/weight parameters,73774,70094 density 0.14210043762327415,0.1350121733234714
before tensor 2359296 mask 76623.0 2359296
layer4.0.conv2.weight, mask/weight parameters,76623,47057 density 0.03312079279307317,0.020340695958963293
before tensor 1048576 mask 121179.0 1048576
layer4.0.conv3.weight, mask/weight parameters,121179,117013 density 0.11670499722633136,0.11269280849358974
layer4.0.shortcut.0.weight, mask/weight parameters,235707,142133 density 0.11239385604858398,0.06777429580688477
before tensor 1048576 mask 153594.0 1048576
layer4.1.conv1.weight, mask/weight parameters,153594,117412 density 0.14792321560650887,0.11307707716962524
before tensor 2359296 mask 503895.0 2359296
layer4.1.conv2.weight, mask/weight parameters,503895,46350 density 0.21781190875410267,0.02003509058584161
before tensor 1048576 mask 188164.0 1048576
layer4.1.conv3.weight, mask/weight parameters,188164,116893 density 0.18121687006903353,0.11257723896696252
before tensor 1048576 mask 127128.0 1048576
layer4.2.conv1.weight, mask/weight parameters,127128,116721 density 0.12243435650887574,0.11241158931213018
before tensor 2359296 mask 112676.0 2359296
layer4.2.conv2.weight, mask/weight parameters,112676,46590 density 0.04870493779612275,0.020138832155218136
before tensor 1048576 mask 129481.0 1048576
layer4.2.conv3.weight, mask/weight parameters,129481,117232 density 0.12470048231015779,0.11290372287968442
classifier.weight, mask/weight parameters,114453,98878 density 0.5588525390625,0.482802734375
Total Model parameters after dst: 3547113 2347917
Total parameters under density level of 0.1: 0.15180338343427202 0.10048220753690273 after dst
Death rate: 0.14997080567080823


Training summary: Average loss: 3.7941, Accuracy: 5209/50000 (10.418%)


Evaluation: Average loss: 3.5870, Accuracy: 1443/10000 (14.430%)

Current learning rate: 0.1. Time taken for epoch: 92.69 seconds.

Train Epoch: 2 [0/50000 (0%)]	Loss: 3.564248 Accuracy: 13/100 (13.000%
Train Epoch: 2 [10000/50000 (20%)]	Loss: 3.739524 Accuracy: 1115/10100 (11.040%
Train Epoch: 2 [20000/50000 (40%)]	Loss: 3.405139 Accuracy: 2468/20100 (12.279%
Train Epoch: 2 [30000/50000 (60%)]	Loss: 3.241635 Accuracy: 4039/30100 (13.419%
Train Epoch: 2 [40000/50000 (80%)]	Loss: 3.580568 Accuracy: 5710/40100 (14.239%
current layer rate 0.03382271279016846
===========del layer===============
===========del layer with layer-wise HE===============
Total filter_names items: 7490.0
Total filter_masks items: 20391071.0
7490.0 / 7552 channels
Total channels to prune: 193.42912699135195
Layer ('layer1', 0, 'conv1'): Pruned 1/64.0 channels based on HE
Layer ('layer1', 0, 'conv2'): Pruned 1/64.0 channels based on HE
Layer ('layer1', 1, 'conv1'): Pruned 1/64.0 channels based on HE
Layer ('layer1', 1, 'conv2'): Pruned 1/64.0 channels based on HE
Layer ('layer1', 2, 'conv1'): Pruned 1/64.0 channels based on HE
Layer ('layer1', 2, 'conv2'): Pruned 1/64.0 channels based on HE
Layer ('layer2', 0, 'conv1'): Pruned 3/127.0 channels based on HE
Layer ('layer2', 0, 'conv2'): Pruned 3/127.0 channels based on HE
Layer ('layer2', 1, 'conv1'): Pruned 3/127.0 channels based on HE
Layer ('layer2', 1, 'conv2'): Pruned 3/127.0 channels based on HE
Layer ('layer2', 2, 'conv1'): Pruned 3/127.0 channels based on HE
Layer ('layer2', 2, 'conv2'): Pruned 3/127.0 channels based on HE
Layer ('layer2', 3, 'conv1'): Pruned 3/127.0 channels based on HE
Layer ('layer2', 3, 'conv2'): Pruned 3/127.0 channels based on HE
Layer ('layer3', 0, 'conv1'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 0, 'conv2'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 1, 'conv1'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 1, 'conv2'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 2, 'conv1'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 2, 'conv2'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 3, 'conv1'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 3, 'conv2'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 4, 'conv1'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 4, 'conv2'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 5, 'conv1'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 5, 'conv2'): Pruned 6/254.0 channels based on HE
Layer ('layer4', 0, 'conv1'): Pruned 13/507.0 channels based on HE
Layer ('layer4', 0, 'conv2'): Pruned 13/507.0 channels based on HE
Layer ('layer4', 1, 'conv1'): Pruned 13/507.0 channels based on HE
Layer ('layer4', 1, 'conv2'): Pruned 13/507.0 channels based on HE
Layer ('layer4', 2, 'conv1'): Pruned 13/507.0 channels based on HE
Layer ('layer4', 2, 'conv2'): Pruned 13/507.0 channels based on HE
update_filter_mask
After update_filter_mask:
Total filter_names items: 7310.0
Total filter_masks items: 19616583.0
After apply_mask:
Total filter_names items: 7310.0
Total filter_masks items: 19616583.0
Total pruned: 180/193.42912699135195 channels
===========done ===============

Training summary: Average loss: 3.5183, Accuracy: 7550/50000 (15.100%)


Evaluation: Average loss: 3.4990, Accuracy: 1589/10000 (15.890%)

Current learning rate: 0.1. Time taken for epoch: 92.09 seconds.

Train Epoch: 3 [0/50000 (0%)]	Loss: 3.167111 Accuracy: 23/100 (23.000%
Train Epoch: 3 [10000/50000 (20%)]	Loss: 3.327249 Accuracy: 2045/10100 (20.248%
Train Epoch: 3 [20000/50000 (40%)]	Loss: 2.949855 Accuracy: 4199/20100 (20.891%
Train Epoch: 3 [30000/50000 (60%)]	Loss: 3.332773 Accuracy: 6370/30100 (21.163%
Train Epoch: 3 [40000/50000 (80%)]	Loss: 3.221926 Accuracy: 8768/40100 (21.865%

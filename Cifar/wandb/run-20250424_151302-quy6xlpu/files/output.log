


================================================================================

Iteration start: 1/1

==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
model ResNet50
Growth mode: global_gradients not supported!
Supported modes are: ['random', 'momentum', 'momentum_neuron', 'gradient']
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (classifier): Linear(in_features=2048, out_features=100, bias=False)
)
add module
Removing biases...
Removed 53 layers.
Removing 2D batch norms...
Removing bn1 of size torch.Size([64]) = 64 parameters.
Removing layer1.0.bn1 of size torch.Size([64]) = 64 parameters.
Removing layer1.0.bn2 of size torch.Size([64]) = 64 parameters.
Removing layer1.0.bn3 of size torch.Size([256]) = 256 parameters.
Removing layer1.0.shortcut.1 of size torch.Size([256]) = 256 parameters.
Removing layer1.1.bn1 of size torch.Size([64]) = 64 parameters.
Removing layer1.1.bn2 of size torch.Size([64]) = 64 parameters.
Removing layer1.1.bn3 of size torch.Size([256]) = 256 parameters.
Removing layer1.2.bn1 of size torch.Size([64]) = 64 parameters.
Removing layer1.2.bn2 of size torch.Size([64]) = 64 parameters.
Removing layer1.2.bn3 of size torch.Size([256]) = 256 parameters.
Removing layer2.0.bn1 of size torch.Size([128]) = 128 parameters.
Removing layer2.0.bn2 of size torch.Size([128]) = 128 parameters.
Removing layer2.0.bn3 of size torch.Size([512]) = 512 parameters.
Removing layer2.0.shortcut.1 of size torch.Size([512]) = 512 parameters.
Removing layer2.1.bn1 of size torch.Size([128]) = 128 parameters.
Removing layer2.1.bn2 of size torch.Size([128]) = 128 parameters.
Removing layer2.1.bn3 of size torch.Size([512]) = 512 parameters.
Removing layer2.2.bn1 of size torch.Size([128]) = 128 parameters.
Removing layer2.2.bn2 of size torch.Size([128]) = 128 parameters.
Removing layer2.2.bn3 of size torch.Size([512]) = 512 parameters.
Removing layer2.3.bn1 of size torch.Size([128]) = 128 parameters.
Removing layer2.3.bn2 of size torch.Size([128]) = 128 parameters.
Removing layer2.3.bn3 of size torch.Size([512]) = 512 parameters.
Removing layer3.0.bn1 of size torch.Size([256]) = 256 parameters.
Removing layer3.0.bn2 of size torch.Size([256]) = 256 parameters.
Removing layer3.0.bn3 of size torch.Size([1024]) = 1024 parameters.
Removing layer3.0.shortcut.1 of size torch.Size([1024]) = 1024 parameters.
Removing layer3.1.bn1 of size torch.Size([256]) = 256 parameters.
Removing layer3.1.bn2 of size torch.Size([256]) = 256 parameters.
Removing layer3.1.bn3 of size torch.Size([1024]) = 1024 parameters.
Removing layer3.2.bn1 of size torch.Size([256]) = 256 parameters.
Removing layer3.2.bn2 of size torch.Size([256]) = 256 parameters.
Removing layer3.2.bn3 of size torch.Size([1024]) = 1024 parameters.
Removing layer3.3.bn1 of size torch.Size([256]) = 256 parameters.
Removing layer3.3.bn2 of size torch.Size([256]) = 256 parameters.
Removing layer3.3.bn3 of size torch.Size([1024]) = 1024 parameters.
Removing layer3.4.bn1 of size torch.Size([256]) = 256 parameters.
Removing layer3.4.bn2 of size torch.Size([256]) = 256 parameters.
Removing layer3.4.bn3 of size torch.Size([1024]) = 1024 parameters.
Removing layer3.5.bn1 of size torch.Size([256]) = 256 parameters.
Removing layer3.5.bn2 of size torch.Size([256]) = 256 parameters.
Removing layer3.5.bn3 of size torch.Size([1024]) = 1024 parameters.
Removing layer4.0.bn1 of size torch.Size([512]) = 512 parameters.
Removing layer4.0.bn2 of size torch.Size([512]) = 512 parameters.
Removing layer4.0.bn3 of size torch.Size([2048]) = 2048 parameters.
Removing layer4.0.shortcut.1 of size torch.Size([2048]) = 2048 parameters.
Removing layer4.1.bn1 of size torch.Size([512]) = 512 parameters.
Removing layer4.1.bn2 of size torch.Size([512]) = 512 parameters.
Removing layer4.1.bn3 of size torch.Size([2048]) = 2048 parameters.
Removing layer4.2.bn1 of size torch.Size([512]) = 512 parameters.
Removing layer4.2.bn2 of size torch.Size([512]) = 512 parameters.
Removing layer4.2.bn3 of size torch.Size([2048]) = 2048 parameters.
Removing 1D batch norms...
baseline fitler num 7552
initialize by fixed_ERK
Sparsity of var:conv1.weight had to be set to 0.
Sparsity of var:layer1.0.conv1.weight had to be set to 0.
layer: conv1.weight, shape: torch.Size([64, 3, 3, 3]), density: 1.0
layer: layer1.0.conv1.weight, shape: torch.Size([64, 64, 1, 1]), density: 1.0
layer: layer1.0.conv2.weight, shape: torch.Size([64, 64, 3, 3]), density: 0.16747319723795276
layer: layer1.0.conv3.weight, shape: torch.Size([256, 64, 1, 1]), density: 0.9054800850663938
layer: layer1.0.shortcut.0.weight, shape: torch.Size([256, 64, 1, 1]), density: 0.9054800850663938
layer: layer1.1.conv1.weight, shape: torch.Size([64, 256, 1, 1]), density: 0.9054800850663938
layer: layer1.1.conv2.weight, shape: torch.Size([64, 64, 3, 3]), density: 0.16747319723795276
layer: layer1.1.conv3.weight, shape: torch.Size([256, 64, 1, 1]), density: 0.9054800850663938
layer: layer1.2.conv1.weight, shape: torch.Size([64, 256, 1, 1]), density: 0.9054800850663938
layer: layer1.2.conv2.weight, shape: torch.Size([64, 64, 3, 3]), density: 0.16747319723795276
layer: layer1.2.conv3.weight, shape: torch.Size([256, 64, 1, 1]), density: 0.9054800850663938
layer: layer2.0.conv1.weight, shape: torch.Size([128, 256, 1, 1]), density: 0.5427256410491118
layer: layer2.0.conv2.weight, shape: torch.Size([128, 128, 3, 3]), density: 0.08186189864989481
layer: layer2.0.conv3.weight, shape: torch.Size([512, 128, 1, 1]), density: 0.4513340175563857
layer: layer2.0.shortcut.0.weight, shape: torch.Size([512, 256, 1, 1]), density: 0.2706598080361503
layer: layer2.1.conv1.weight, shape: torch.Size([128, 512, 1, 1]), density: 0.4513340175563857
layer: layer2.1.conv2.weight, shape: torch.Size([128, 128, 3, 3]), density: 0.08186189864989481
layer: layer2.1.conv3.weight, shape: torch.Size([512, 128, 1, 1]), density: 0.4513340175563857
layer: layer2.2.conv1.weight, shape: torch.Size([128, 512, 1, 1]), density: 0.4513340175563857
layer: layer2.2.conv2.weight, shape: torch.Size([128, 128, 3, 3]), density: 0.08186189864989481
layer: layer2.2.conv3.weight, shape: torch.Size([512, 128, 1, 1]), density: 0.4513340175563857
layer: layer2.3.conv1.weight, shape: torch.Size([128, 512, 1, 1]), density: 0.4513340175563857
layer: layer2.3.conv2.weight, shape: torch.Size([128, 128, 3, 3]), density: 0.08186189864989481
layer: layer2.3.conv3.weight, shape: torch.Size([512, 128, 1, 1]), density: 0.4513340175563857
layer: layer3.0.conv1.weight, shape: torch.Size([256, 512, 1, 1]), density: 0.2706598080361503
layer: layer3.0.conv2.weight, shape: torch.Size([256, 256, 3, 3]), density: 0.04046227433267702
layer: layer3.0.conv3.weight, shape: torch.Size([1024, 256, 1, 1]), density: 0.22531550253399008
layer: layer3.0.shortcut.0.weight, shape: torch.Size([1024, 512, 1, 1]), density: 0.13515415089597377
layer: layer3.1.conv1.weight, shape: torch.Size([256, 1024, 1, 1]), density: 0.22531550253399008
layer: layer3.1.conv2.weight, shape: torch.Size([256, 256, 3, 3]), density: 0.04046227433267702
layer: layer3.1.conv3.weight, shape: torch.Size([1024, 256, 1, 1]), density: 0.22531550253399008
layer: layer3.2.conv1.weight, shape: torch.Size([256, 1024, 1, 1]), density: 0.22531550253399008
layer: layer3.2.conv2.weight, shape: torch.Size([256, 256, 3, 3]), density: 0.04046227433267702
layer: layer3.2.conv3.weight, shape: torch.Size([1024, 256, 1, 1]), density: 0.22531550253399008
layer: layer3.3.conv1.weight, shape: torch.Size([256, 1024, 1, 1]), density: 0.22531550253399008
layer: layer3.3.conv2.weight, shape: torch.Size([256, 256, 3, 3]), density: 0.04046227433267702
layer: layer3.3.conv3.weight, shape: torch.Size([1024, 256, 1, 1]), density: 0.22531550253399008
layer: layer3.4.conv1.weight, shape: torch.Size([256, 1024, 1, 1]), density: 0.22531550253399008
layer: layer3.4.conv2.weight, shape: torch.Size([256, 256, 3, 3]), density: 0.04046227433267702
layer: layer3.4.conv3.weight, shape: torch.Size([1024, 256, 1, 1]), density: 0.22531550253399008
layer: layer3.5.conv1.weight, shape: torch.Size([256, 1024, 1, 1]), density: 0.22531550253399008
layer: layer3.5.conv2.weight, shape: torch.Size([256, 256, 3, 3]), density: 0.04046227433267702
layer: layer3.5.conv3.weight, shape: torch.Size([1024, 256, 1, 1]), density: 0.22531550253399008
layer: layer4.0.conv1.weight, shape: torch.Size([512, 1024, 1, 1]), density: 0.13515415089597377
layer: layer4.0.conv2.weight, shape: torch.Size([512, 512, 3, 3]), density: 0.02011396841827091
layer: layer4.0.conv3.weight, shape: torch.Size([2048, 512, 1, 1]), density: 0.11256987470594433
layer: layer4.0.shortcut.0.weight, shape: torch.Size([2048, 1024, 1, 1]), density: 0.06753313716746152
layer: layer4.1.conv1.weight, shape: torch.Size([512, 2048, 1, 1]), density: 0.11256987470594433
layer: layer4.1.conv2.weight, shape: torch.Size([512, 512, 3, 3]), density: 0.02011396841827091
layer: layer4.1.conv3.weight, shape: torch.Size([2048, 512, 1, 1]), density: 0.11256987470594433
layer: layer4.2.conv1.weight, shape: torch.Size([512, 2048, 1, 1]), density: 0.11256987470594433
layer: layer4.2.conv2.weight, shape: torch.Size([512, 512, 3, 3]), density: 0.02011396841827091
layer: layer4.2.conv3.weight, shape: torch.Size([2048, 512, 1, 1]), density: 0.11256987470594433
layer: classifier.weight, shape: torch.Size([100, 2048]), density: 0.48322266403046304
Overall sparsity 0.10000000000000005
conv1.weight 1728
layer1.0.conv1.weight 4096
layer1.0.conv2.weight 36864
layer1.0.conv3.weight 16384
layer1.0.shortcut.0.weight 16384
layer1.1.conv1.weight 16384
layer1.1.conv2.weight 36864
layer1.1.conv3.weight 16384
layer1.2.conv1.weight 16384
layer1.2.conv2.weight 36864
layer1.2.conv3.weight 16384
layer2.0.conv1.weight 32768
layer2.0.conv2.weight 147456
layer2.0.conv3.weight 65536
layer2.0.shortcut.0.weight 131072
layer2.1.conv1.weight 65536
layer2.1.conv2.weight 147456
layer2.1.conv3.weight 65536
layer2.2.conv1.weight 65536
layer2.2.conv2.weight 147456
layer2.2.conv3.weight 65536
layer2.3.conv1.weight 65536
layer2.3.conv2.weight 147456
layer2.3.conv3.weight 65536
layer3.0.conv1.weight 131072
layer3.0.conv2.weight 589824
layer3.0.conv3.weight 262144
layer3.0.shortcut.0.weight 524288
layer3.1.conv1.weight 262144
layer3.1.conv2.weight 589824
layer3.1.conv3.weight 262144
layer3.2.conv1.weight 262144
layer3.2.conv2.weight 589824
layer3.2.conv3.weight 262144
layer3.3.conv1.weight 262144
layer3.3.conv2.weight 589824
layer3.3.conv3.weight 262144
layer3.4.conv1.weight 262144
layer3.4.conv2.weight 589824
layer3.4.conv3.weight 262144
layer3.5.conv1.weight 262144
layer3.5.conv2.weight 589824
layer3.5.conv3.weight 262144
layer4.0.conv1.weight 524288
layer4.0.conv2.weight 2359296
layer4.0.conv3.weight 1048576
layer4.0.shortcut.0.weight 2097152
layer4.1.conv1.weight 1048576
layer4.1.conv2.weight 2359296
layer4.1.conv3.weight 1048576
layer4.2.conv1.weight 1048576
layer4.2.conv2.weight 2359296
layer4.2.conv3.weight 1048576
classifier.weight 204800
Total Model parameters after init: 2367257 23652032
Total parameters under sparsity level of 0.1: 0.10008683397688621
save_dir ./Chase_models/saved_models/test/density_0.1/stop_gmp_epochs_130/epoch_160/layer_interval_1000/start_layer_rate_0.5
=====================================
begin pre training
Train Epoch: 0 [0/50000 (0%)]	Loss: 4.644522 Accuracy: 0/100 (0.000%
Train Epoch: 0 [10000/50000 (20%)]	Loss: 4.610788 Accuracy: 113/10100 (1.119%
Train Epoch: 0 [20000/50000 (40%)]	Loss: 4.499642 Accuracy: 307/20100 (1.527%
Train Epoch: 0 [30000/50000 (60%)]	Loss: 4.324152 Accuracy: 672/30100 (2.233%
Train Epoch: 0 [40000/50000 (80%)]	Loss: 4.132882 Accuracy: 1192/40100 (2.973%
current layer rate 0.011449931725079632
===========del layer===============
===========del layer with layer-wise HE===============
Total filter_names items: 7552.0
Total filter_masks items: 20676608.0
7552.0 / 7552 channels
Total channels to prune: 86.46988438780136
Layer ('layer2', 0, 'conv1'): Pruned 1/128.0 channels based on HE
Layer ('layer2', 0, 'conv2'): Pruned 1/128.0 channels based on HE
Layer ('layer2', 1, 'conv1'): Pruned 1/128.0 channels based on HE
Layer ('layer2', 1, 'conv2'): Pruned 1/128.0 channels based on HE
Layer ('layer2', 2, 'conv1'): Pruned 1/128.0 channels based on HE
Layer ('layer2', 2, 'conv2'): Pruned 1/128.0 channels based on HE
Layer ('layer2', 3, 'conv1'): Pruned 1/128.0 channels based on HE
Layer ('layer2', 3, 'conv2'): Pruned 1/128.0 channels based on HE
Layer ('layer3', 0, 'conv1'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 0, 'conv2'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 1, 'conv1'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 1, 'conv2'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 2, 'conv1'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 2, 'conv2'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 3, 'conv1'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 3, 'conv2'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 4, 'conv1'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 4, 'conv2'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 5, 'conv1'): Pruned 2/256.0 channels based on HE
Layer ('layer3', 5, 'conv2'): Pruned 2/256.0 channels based on HE
Layer ('layer4', 0, 'conv1'): Pruned 5/512.0 channels based on HE
Layer ('layer4', 0, 'conv2'): Pruned 5/512.0 channels based on HE
Layer ('layer4', 1, 'conv1'): Pruned 5/512.0 channels based on HE
Layer ('layer4', 1, 'conv2'): Pruned 5/512.0 channels based on HE
Layer ('layer4', 2, 'conv1'): Pruned 5/512.0 channels based on HE
Layer ('layer4', 2, 'conv2'): Pruned 5/512.0 channels based on HE
update_filter_mask
After update_filter_mask:
Total filter_names items: 7490.0
Total filter_masks items: 20391071.0
After apply_mask:
Total filter_names items: 7490.0
Total filter_masks items: 20391071.0
Total pruned: 62/86.46988438780136 channels
===========done ===============

Training summary: Average loss: 4.4733, Accuracy: 1805/50000 (3.610%)


Evaluation: Average loss: 3.9953, Accuracy: 735/10000 (7.350%)

Current learning rate: 0.1. Time taken for epoch: 92.81 seconds.

Train Epoch: 1 [0/50000 (0%)]	Loss: 3.882050 Accuracy: 13/100 (13.000%
Train Epoch: 1 [10000/50000 (20%)]	Loss: 3.840089 Accuracy: 825/10100 (8.168%
Train Epoch: 1 [20000/50000 (40%)]	Loss: 3.813128 Accuracy: 1809/20100 (9.000%
Train Epoch: 1 [30000/50000 (60%)]	Loss: 3.529928 Accuracy: 2974/30100 (9.880%
Train Epoch: 1 [40000/50000 (80%)]	Loss: 3.631164 Accuracy: 4212/40100 (10.504%
current mest  death_rate 0.14997080567080823


dynamic sparse change prune
to kill -17582.200000001118 expect 2365203.200000001
conv1.weight, mask/weight parameters,1728,1728 density 1.0,1.0
before tensor 4096 mask 4096.0 4096
layer1.0.conv1.weight, mask/weight parameters,4096,4096 density 1.0,1.0
before tensor 36864 mask 6206.0 36864
layer1.0.conv2.weight, mask/weight parameters,6206,6206 density 0.16834852430555555,0.16834852430555555
before tensor 16384 mask 14835.0 16384
layer1.0.conv3.weight, mask/weight parameters,14835,14835 density 0.90545654296875,0.90545654296875
layer1.0.shortcut.0.weight, mask/weight parameters,14829,14829 density 0.90509033203125,0.90509033203125
before tensor 16384 mask 14823.0 16384
layer1.1.conv1.weight, mask/weight parameters,14823,14823 density 0.90472412109375,0.90472412109375
before tensor 36864 mask 6155.0 36864
layer1.1.conv2.weight, mask/weight parameters,6155,6155 density 0.1669650607638889,0.1669650607638889
before tensor 16384 mask 14857.0 16384
layer1.1.conv3.weight, mask/weight parameters,14857,14857 density 0.90679931640625,0.90679931640625
before tensor 16384 mask 14849.0 16384
layer1.2.conv1.weight, mask/weight parameters,14849,14849 density 0.90631103515625,0.90631103515625
before tensor 36864 mask 6104.0 36864
layer1.2.conv2.weight, mask/weight parameters,6104,6104 density 0.1655815972222222,0.1655815972222222
before tensor 16384 mask 14785.0 16384
layer1.2.conv3.weight, mask/weight parameters,14785,14785 density 0.90240478515625,0.90240478515625
before tensor 32768 mask 17693.0 32768
layer2.0.conv1.weight, mask/weight parameters,17693,17693 density 0.54419906496063,0.54419906496063
before tensor 147456 mask 11927.0 147456
layer2.0.conv2.weight, mask/weight parameters,11927,11927 density 0.08216394210566198,0.08216394210566198
before tensor 65536 mask 29279.0 65536
layer2.0.conv3.weight, mask/weight parameters,29279,29279 density 0.4502798966535433,0.4502798966535433
layer2.0.shortcut.0.weight, mask/weight parameters,35383,35383 density 0.26995086669921875,0.26995086669921875
before tensor 65536 mask 29277.0 65536
layer2.1.conv1.weight, mask/weight parameters,29277,29277 density 0.45024913877952755,0.45024913877952755
before tensor 147456 mask 11904.0 147456
layer2.1.conv2.weight, mask/weight parameters,11904,11904 density 0.08200549734432802,0.08200549734432802
before tensor 65536 mask 29363.0 65536
layer2.1.conv3.weight, mask/weight parameters,29363,29363 density 0.45157172736220474,0.45157172736220474
before tensor 65536 mask 29582.0 65536
layer2.2.conv1.weight, mask/weight parameters,29582,29582 density 0.4549397145669291,0.4549397145669291
before tensor 147456 mask 11907.0 147456
layer2.2.conv2.weight, mask/weight parameters,11907,11907 density 0.0820261640523281,0.0820261640523281
before tensor 65536 mask 29267.0 65536
layer2.2.conv3.weight, mask/weight parameters,29267,29267 density 0.45009534940944884,0.45009534940944884
before tensor 65536 mask 29344.0 65536
layer2.3.conv1.weight, mask/weight parameters,29344,29344 density 0.4512795275590551,0.4512795275590551
before tensor 147456 mask 11945.0 147456
layer2.3.conv2.weight, mask/weight parameters,11945,11945 density 0.08228794235366249,0.08228794235366249
before tensor 65536 mask 29462.0 65536
layer2.3.conv3.weight, mask/weight parameters,29462,29462 density 0.45309424212598426,0.45309424212598426
before tensor 131072 mask 35259.0 131072
layer3.0.conv1.weight, mask/weight parameters,35259,35259 density 0.271122969980315,0.271122969980315
before tensor 589824 mask 23550.0 589824
layer3.0.conv2.weight, mask/weight parameters,23550,23550 density 0.040558414450162235,0.040558414450162235
before tensor 262144 mask 58665.0 262144
layer3.0.conv3.weight, mask/weight parameters,58665,58665 density 0.2255513348917323,0.2255513348917323
layer3.0.shortcut.0.weight, mask/weight parameters,70817,70817 density 0.1350727081298828,0.1350727081298828
before tensor 262144 mask 58935.0 262144
layer3.1.conv1.weight, mask/weight parameters,58935,58935 density 0.22658941313976377,0.22658941313976377
before tensor 589824 mask 23649.0 589824
layer3.1.conv2.weight, mask/weight parameters,23649,23649 density 0.04072891479116292,0.04072891479116292
before tensor 262144 mask 58440.0 262144
layer3.1.conv3.weight, mask/weight parameters,58440,58440 density 0.22468626968503938,0.22468626968503938
before tensor 262144 mask 58179.0 262144
layer3.2.conv1.weight, mask/weight parameters,58179,58179 density 0.22368279404527558,0.22368279404527558
before tensor 589824 mask 23141.0 589824
layer3.2.conv2.weight, mask/weight parameters,23141,23141 density 0.03985402415249275,0.03985402415249275
before tensor 262144 mask 58790.0 262144
layer3.2.conv3.weight, mask/weight parameters,58790,58790 density 0.22603192667322836,0.22603192667322836
before tensor 262144 mask 58694.0 262144
layer3.3.conv1.weight, mask/weight parameters,58694,58694 density 0.22566283218503938,0.22566283218503938
before tensor 589824 mask 23628.0 589824
layer3.3.conv2.weight, mask/weight parameters,23628,23628 density 0.04069274805216277,0.04069274805216277
before tensor 262144 mask 58951.0 262144
layer3.3.conv3.weight, mask/weight parameters,58951,58951 density 0.22665092888779528,0.22665092888779528
before tensor 262144 mask 58572.0 262144
layer3.4.conv1.weight, mask/weight parameters,58572,58572 density 0.22519377460629922,0.22519377460629922
before tensor 589824 mask 23560.0 589824
layer3.4.conv2.weight, mask/weight parameters,23560,23560 density 0.04057563670682897,0.04057563670682897
before tensor 262144 mask 58095.0 262144
layer3.4.conv3.weight, mask/weight parameters,58095,58095 density 0.22335983636811024,0.22335983636811024
before tensor 262144 mask 58623.0 262144
layer3.5.conv1.weight, mask/weight parameters,58623,58623 density 0.2253898560531496,0.2253898560531496
before tensor 589824 mask 23550.0 589824
layer3.5.conv2.weight, mask/weight parameters,23550,23550 density 0.040558414450162235,0.040558414450162235
before tensor 262144 mask 58625.0 262144
layer3.5.conv3.weight, mask/weight parameters,58625,58625 density 0.22539754552165353,0.22539754552165353
before tensor 524288 mask 70047.0 524288
layer4.0.conv1.weight, mask/weight parameters,70047,70047 density 0.13492164386094674,0.13492164386094674
before tensor 2359296 mask 47038.0 2359296
layer4.0.conv2.weight, mask/weight parameters,47038,47038 density 0.020332483084720986,0.020332483084720986
before tensor 1048576 mask 116974.0 1048576
layer4.0.conv3.weight, mask/weight parameters,116974,116974 density 0.1126552483974359,0.1126552483974359
layer4.0.shortcut.0.weight, mask/weight parameters,142133,142133 density 0.06777429580688477,0.06777429580688477
before tensor 1048576 mask 117451.0 1048576
layer4.1.conv1.weight, mask/weight parameters,117451,117451 density 0.1131146372657791,0.1131146372657791
before tensor 2359296 mask 46317.0 2359296
layer4.1.conv2.weight, mask/weight parameters,46317,46317 density 0.020020826120052338,0.020020826120052338
before tensor 1048576 mask 116867.0 1048576
layer4.1.conv3.weight, mask/weight parameters,116867,116867 density 0.11255219890285996,0.11255219890285996
before tensor 1048576 mask 116768.0 1048576
layer4.2.conv1.weight, mask/weight parameters,116768,116768 density 0.1124568540433925,0.1124568540433925
before tensor 2359296 mask 46632.0 2359296
layer4.2.conv2.weight, mask/weight parameters,46632,46632 density 0.02015698692985903,0.02015698692985903
before tensor 1048576 mask 117193.0 1048576
layer4.2.conv3.weight, mask/weight parameters,117193,117193 density 0.11286616278353057,0.11286616278353057
classifier.weight, mask/weight parameters,98878,98878 density 0.482802734375,0.482802734375
Total Model parameters after dst: 2347621 2347621
Total parameters under density level of 0.1: 0.10046953982614851 0.10046953982614851 after dst
Death rate: 0.14997080567080823

dynamic sparse change grow
self.total_params*pruning_rate 3547114.2947917376
self.total_nonzero 2347621.0
to grow 1199493.2947917376
len(all_scores) 23366495 21018332
increse 1199493.0 before_mask 3547113.0 after_mask 3547113.0
total_nonzero_new None
conv1.weight, mask/weight parameters,1728,1728 density 1.0,1.0
before tensor 4096 mask 4096.0 4096
layer1.0.conv1.weight, mask/weight parameters,4096,4096 density 1.0,1.0
before tensor 36864 mask 7998.0 36864
layer1.0.conv2.weight, mask/weight parameters,7998,6206 density 0.21695963541666666,0.16834852430555555
before tensor 16384 mask 14949.0 16384
layer1.0.conv3.weight, mask/weight parameters,14949,14835 density 0.91241455078125,0.90545654296875
layer1.0.shortcut.0.weight, mask/weight parameters,14956,14829 density 0.912841796875,0.90509033203125
before tensor 16384 mask 14878.0 16384
layer1.1.conv1.weight, mask/weight parameters,14878,14823 density 0.9080810546875,0.90472412109375
before tensor 36864 mask 7218.0 36864
layer1.1.conv2.weight, mask/weight parameters,7218,6155 density 0.19580078125,0.1669650607638889
before tensor 16384 mask 14914.0 16384
layer1.1.conv3.weight, mask/weight parameters,14914,14857 density 0.9102783203125,0.90679931640625
before tensor 16384 mask 14876.0 16384
layer1.2.conv1.weight, mask/weight parameters,14876,14849 density 0.907958984375,0.90631103515625
before tensor 36864 mask 6887.0 36864
layer1.2.conv2.weight, mask/weight parameters,6887,6104 density 0.1868218315972222,0.1655815972222222
before tensor 16384 mask 14803.0 16384
layer1.2.conv3.weight, mask/weight parameters,14803,14785 density 0.90350341796875,0.90240478515625
before tensor 32768 mask 18412.0 32768
layer2.0.conv1.weight, mask/weight parameters,18412,17693 density 0.5663139763779528,0.54419906496063
before tensor 147456 mask 19681.0 147456
layer2.0.conv2.weight, mask/weight parameters,19681,11927 density 0.135580493383209,0.08216394210566198
before tensor 65536 mask 30553.0 65536
layer2.0.conv3.weight, mask/weight parameters,30553,29279 density 0.4698726624015748,0.4502798966535433
layer2.0.shortcut.0.weight, mask/weight parameters,40055,35383 density 0.30559539794921875,0.26995086669921875
before tensor 65536 mask 29924.0 65536
layer2.1.conv1.weight, mask/weight parameters,29924,29277 density 0.46019931102362205,0.45024913877952755
before tensor 147456 mask 20210.0 147456
layer2.1.conv2.weight, mask/weight parameters,20210,11904 density 0.13922472289389023,0.08200549734432802
before tensor 65536 mask 30524.0 65536
layer2.1.conv3.weight, mask/weight parameters,30524,29363 density 0.46942667322834647,0.45157172736220474
before tensor 65536 mask 30097.0 65536
layer2.2.conv1.weight, mask/weight parameters,30097,29582 density 0.46285986712598426,0.4549397145669291
before tensor 147456 mask 17556.0 147456
layer2.2.conv2.weight, mask/weight parameters,17556,11907 density 0.12094157521648377,0.0820261640523281
before tensor 65536 mask 30114.0 65536
layer2.2.conv3.weight, mask/weight parameters,30114,29267 density 0.4631213090551181,0.45009534940944884
before tensor 65536 mask 29848.0 65536
layer2.3.conv1.weight, mask/weight parameters,29848,29344 density 0.4590305118110236,0.4512795275590551
before tensor 147456 mask 16249.0 147456
layer2.3.conv2.weight, mask/weight parameters,16249,11945 density 0.11193777943111442,0.08228794235366249
before tensor 65536 mask 30023.0 65536
layer2.3.conv3.weight, mask/weight parameters,30023,29462 density 0.46172182578740156,0.45309424212598426
before tensor 131072 mask 39159.0 131072
layer3.0.conv1.weight, mask/weight parameters,39159,35259 density 0.30111189714566927,0.271122969980315
before tensor 589824 mask 53868.0 589824
layer3.0.conv2.weight, mask/weight parameters,53868,23550 density 0.0927728522123711,0.040558414450162235
before tensor 262144 mask 63602.0 262144
layer3.0.conv3.weight, mask/weight parameters,63602,58665 density 0.24453278789370078,0.2255513348917323
layer3.0.shortcut.0.weight, mask/weight parameters,93190,70817 density 0.17774581909179688,0.1350727081298828
before tensor 262144 mask 63801.0 262144
layer3.1.conv1.weight, mask/weight parameters,63801,58935 density 0.2452978900098425,0.22658941313976377
before tensor 589824 mask 89836.0 589824
layer3.1.conv2.weight, mask/weight parameters,89836,23649 density 0.15471786499128554,0.04072891479116292
before tensor 262144 mask 69138.0 262144
layer3.1.conv3.weight, mask/weight parameters,69138,58440 density 0.26581723671259844,0.22468626968503938
before tensor 262144 mask 61820.0 262144
layer3.2.conv1.weight, mask/weight parameters,61820,58179 density 0.2376814714566929,0.22368279404527558
before tensor 589824 mask 60040.0 589824
layer3.2.conv2.weight, mask/weight parameters,60040,23141 density 0.10340242902708027,0.03985402415249275
before tensor 262144 mask 63634.0 262144
layer3.2.conv3.weight, mask/weight parameters,63634,58790 density 0.24465581938976377,0.22603192667322836
before tensor 262144 mask 62102.0 262144
layer3.3.conv1.weight, mask/weight parameters,62102,58694 density 0.23876568651574803,0.22566283218503938
before tensor 589824 mask 58999.0 589824
layer3.3.conv2.weight, mask/weight parameters,58999,23628 density 0.1016095921080731,0.04069274805216277
before tensor 262144 mask 62537.0 262144
layer3.3.conv3.weight, mask/weight parameters,62537,58951 density 0.24043814591535434,0.22665092888779528
before tensor 262144 mask 59853.0 262144
layer3.4.conv1.weight, mask/weight parameters,59853,58572 density 0.23011887918307086,0.22519377460629922
before tensor 589824 mask 39189.0 589824
layer3.4.conv2.weight, mask/weight parameters,39189,23560 density 0.06749230165126997,0.04057563670682897
before tensor 262144 mask 59802.0 262144
layer3.4.conv3.weight, mask/weight parameters,59802,58095 density 0.22992279773622049,0.22335983636811024
before tensor 262144 mask 60225.0 262144
layer3.5.conv1.weight, mask/weight parameters,60225,58623 density 0.23154912032480315,0.2253898560531496
before tensor 589824 mask 41555.0 589824
layer3.5.conv2.weight, mask/weight parameters,41555,23550 density 0.0715670875786196,0.040558414450162235
before tensor 262144 mask 59852.0 262144
layer3.5.conv3.weight, mask/weight parameters,59852,58625 density 0.2301150344488189,0.22539754552165353
before tensor 524288 mask 76302.0 524288
layer4.0.conv1.weight, mask/weight parameters,76302,70047 density 0.1469697670118343,0.13492164386094674
before tensor 2359296 mask 92986.0 2359296
layer4.0.conv2.weight, mask/weight parameters,92986,47038 density 0.04019380654185691,0.020332483084720986
before tensor 1048576 mask 125014.0 1048576
layer4.0.conv3.weight, mask/weight parameters,125014,116974 density 0.12039840668145957,0.1126552483974359
layer4.0.shortcut.0.weight, mask/weight parameters,241508,142133 density 0.11515998840332031,0.06777429580688477
before tensor 1048576 mask 162762.0 1048576
layer4.1.conv1.weight, mask/weight parameters,162762,117451 density 0.1567527274408284,0.1131146372657791
before tensor 2359296 mask 559553.0 2359296
layer4.1.conv2.weight, mask/weight parameters,559553,46317 density 0.2418704432055972,0.020020826120052338
before tensor 1048576 mask 196834.0 1048576
layer4.1.conv3.weight, mask/weight parameters,196834,116867 density 0.1895667683678501,0.11255219890285996
before tensor 1048576 mask 125505.0 1048576
layer4.2.conv1.weight, mask/weight parameters,125505,116768 density 0.1208712786612426,0.1124568540433925
before tensor 2359296 mask 99526.0 2359296
layer4.2.conv2.weight, mask/weight parameters,99526,46632 density 0.04302076430736725,0.02015698692985903
before tensor 1048576 mask 128772.0 1048576
layer4.2.conv3.weight, mask/weight parameters,128772,117193 density 0.12401765902366864,0.11286616278353057
classifier.weight, mask/weight parameters,115600,98878 density 0.564453125,0.482802734375
Total Model parameters after dst: 3547113 2347621
Total parameters under density level of 0.1: 0.15180338343427202 0.10046953982614851 after dst
Death rate: 0.14997080567080823


Training summary: Average loss: 3.7673, Accuracy: 5541/50000 (11.082%)


Evaluation: Average loss: 3.5400, Accuracy: 1557/10000 (15.570%)

Current learning rate: 0.1. Time taken for epoch: 90.56 seconds.

Train Epoch: 2 [0/50000 (0%)]	Loss: 3.450584 Accuracy: 17/100 (17.000%
Train Epoch: 2 [10000/50000 (20%)]	Loss: 3.682111 Accuracy: 1203/10100 (11.911%
Train Epoch: 2 [20000/50000 (40%)]	Loss: 3.346915 Accuracy: 2696/20100 (13.413%
Train Epoch: 2 [30000/50000 (60%)]	Loss: 3.175794 Accuracy: 4480/30100 (14.884%
Train Epoch: 2 [40000/50000 (80%)]	Loss: 3.344744 Accuracy: 6310/40100 (15.736%
current layer rate 0.03382271279016846
===========del layer===============
===========del layer with layer-wise HE===============
Total filter_names items: 7490.0
Total filter_masks items: 20391071.0
7490.0 / 7552 channels
Total channels to prune: 193.42912699135195
Layer ('layer1', 0, 'conv1'): Pruned 1/64.0 channels based on HE
Layer ('layer1', 0, 'conv2'): Pruned 1/64.0 channels based on HE
Layer ('layer1', 1, 'conv1'): Pruned 1/64.0 channels based on HE
Layer ('layer1', 1, 'conv2'): Pruned 1/64.0 channels based on HE
Layer ('layer1', 2, 'conv1'): Pruned 1/64.0 channels based on HE
Layer ('layer1', 2, 'conv2'): Pruned 1/64.0 channels based on HE
Layer ('layer2', 0, 'conv1'): Pruned 3/127.0 channels based on HE
Layer ('layer2', 0, 'conv2'): Pruned 3/127.0 channels based on HE
Layer ('layer2', 1, 'conv1'): Pruned 3/127.0 channels based on HE
Layer ('layer2', 1, 'conv2'): Pruned 3/127.0 channels based on HE
Layer ('layer2', 2, 'conv1'): Pruned 3/127.0 channels based on HE
Layer ('layer2', 2, 'conv2'): Pruned 3/127.0 channels based on HE
Layer ('layer2', 3, 'conv1'): Pruned 3/127.0 channels based on HE
Layer ('layer2', 3, 'conv2'): Pruned 3/127.0 channels based on HE
Layer ('layer3', 0, 'conv1'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 0, 'conv2'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 1, 'conv1'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 1, 'conv2'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 2, 'conv1'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 2, 'conv2'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 3, 'conv1'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 3, 'conv2'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 4, 'conv1'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 4, 'conv2'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 5, 'conv1'): Pruned 6/254.0 channels based on HE
Layer ('layer3', 5, 'conv2'): Pruned 6/254.0 channels based on HE
Layer ('layer4', 0, 'conv1'): Pruned 13/507.0 channels based on HE
Layer ('layer4', 0, 'conv2'): Pruned 13/507.0 channels based on HE
Layer ('layer4', 1, 'conv1'): Pruned 13/507.0 channels based on HE
Layer ('layer4', 1, 'conv2'): Pruned 13/507.0 channels based on HE
Layer ('layer4', 2, 'conv1'): Pruned 13/507.0 channels based on HE
Layer ('layer4', 2, 'conv2'): Pruned 13/507.0 channels based on HE
update_filter_mask
After update_filter_mask:
Total filter_names items: 7310.0
Total filter_masks items: 19616583.0
After apply_mask:
Total filter_names items: 7310.0
Total filter_masks items: 19616583.0
Total pruned: 180/193.42912699135195 channels
===========done ===============

Training summary: Average loss: 3.4413, Accuracy: 8296/50000 (16.592%)


Evaluation: Average loss: 3.6182, Accuracy: 1412/10000 (14.120%)

Current learning rate: 0.1. Time taken for epoch: 91.19 seconds.

Train Epoch: 3 [0/50000 (0%)]	Loss: 3.158740 Accuracy: 22/100 (22.000%
Train Epoch: 3 [10000/50000 (20%)]	Loss: 3.273849 Accuracy: 2249/10100 (22.267%
Train Epoch: 3 [20000/50000 (40%)]	Loss: 2.656084 Accuracy: 4574/20100 (22.756%
Train Epoch: 3 [30000/50000 (60%)]	Loss: 3.181438 Accuracy: 7005/30100 (23.272%
Train Epoch: 3 [40000/50000 (80%)]	Loss: 3.108406 Accuracy: 9640/40100 (24.040%
